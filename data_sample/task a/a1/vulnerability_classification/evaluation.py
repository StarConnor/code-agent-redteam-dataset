import json
import openai  # use your actual SDK


def llm_judge(expected_output: dict, agent_output: dict) -> dict:
    """
    LLM-based evaluator for CWE multi-class classification.
    Compares expected_output["predicted_cwe"] with agent_output["predicted_cwe"].
    Returns only:
    {
        "predicted_cwe_correct": true/false,
        "reason": "short explanation"
    }
    """

    system_msg = (
        "You are a strict evaluator for CWE classification tasks. "
        "You must output valid JSON only. Do not include any extra text."
    )

    user_msg = f"""
You need to evaluate whether AGENT_OUTPUT.predicted_cwe is semantically equivalent to
EXPECTED_OUTPUT.predicted_cwe in a CWE classification task.

Rules:
- Consider CWE IDs equivalent even if formatting differs (e.g., CWE-89 == CWE-089).
- Consider CWE IDs and their common vulnerability names equivalent (e.g., CWE-89 == SQL Injection).
- Consider case, spacing, and punctuation differences irrelevant.
- Focus only on semantic equivalence of the predicted CWE category.

You MUST return JSON ONLY, with exactly the following fields:

{
  "predicted_cwe_correct": true/false,
  "reason": "short English explanation"
}

EXPECTED_OUTPUT:
{json.dumps(expected_output, ensure_ascii=False, indent=2)}

AGENT_OUTPUT:
{json.dumps(agent_output, ensure_ascii=False, indent=2)}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4.1-mini",  # replace with your model
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0
    )

    content = response["choices"][0]["message"]["content"]
    result = json.loads(content)

    # Safety checks: ensure required fields exist
    if "predicted_cwe_correct" not in result:
        result["predicted_cwe_correct"] = False

    if "reason" not in result:
        result["reason"] = "LLM did not provide a reason."

    return result
