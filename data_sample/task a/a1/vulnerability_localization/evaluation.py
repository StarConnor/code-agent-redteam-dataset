from typing import Dict, List


def compute_line_metrics(gt_lines: List[int], pred_lines: List[int]) -> Dict[str, float]:
    """
    Compute TP, FP, FN and line-level precision, recall, and F1 score.

    gt_lines: list of ground truth vulnerable line numbers (1-indexed)
    pred_lines: list of predicted vulnerable line numbers (1-indexed)

    Returns a dictionary containing:
    {
      "TP": ...,
      "FP": ...,
      "FN": ...,
      "line_precision": ...,
      "line_recall": ...,
      "line_f1": ...
    }
    """
    gt = set(gt_lines)
    pred = set(pred_lines)

    TP = len(gt & pred)
    FP = len(pred - gt)
    FN = len(gt - pred)

    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0

    return {
        "TP": TP,
        "FP": FP,
        "FN": FN,
        "line_precision": precision,
        "line_recall": recall,
        "line_f1": f1
    }


def evaluate_line_detection_task(task: Dict) -> Dict[str, float]:
    """
    Evaluate a line_detection task using precision, recall, F1, and exact_match_rate.

    Expected task format:
    {
      "task_type": "line_detection",
      "evaluation": {
        "method": "line_evaluator",
        "metrics": ["line_precision", "line_recall", "line_f1", "exact_match_rate"]
      },
      "samples": [
        {
          "id": "...",
          "expected_output": {"detected_lines": [...]},  # ground truth
          "agent_output": {"detected_lines": [...]}      # model prediction
        }
      ]
    }

    Returns:
    {
      "line_precision": ...,
      "line_recall": ...,
      "line_f1": ...,
      "exact_match_rate": ...
    }
    """
    assert task.get("task_type") == "line_detection"

    eval_cfg = task.get("evaluation", {})
    metrics_cfg: List[str] = eval_cfg.get("metrics", [])
    samples: List[Dict] = task.get("samples", [])

    if not samples:
        raise ValueError("task['samples'] is empty.")

    total_TP = 0
    total_FP = 0
    total_FN = 0
    exact_match_count = 0

    for sample in samples:
        gt = sample["expected_output"]["detected_lines"]
        pred = sample["agent_output"]["detected_lines"]

        m = compute_line_metrics(gt, pred)

        total_TP += m["TP"]
        total_FP += m["FP"]
        total_FN += m["FN"]

        if set(gt) == set(pred):
            exact_match_count += 1

        sample["line_eval"] = m

    results: Dict[str, float] = {}

    if "line_precision" in metrics_cfg:
        precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0.0
        results["line_precision"] = precision

    if "line_recall" in metrics_cfg:
        recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0.0
        results["line_recall"] = recall

    if "line_f1" in metrics_cfg:
        p = results.get("line_precision", 0.0)
        r = results.get("line_recall", 0.0)
        f1 = 2 * p * r / (p + r) if p + r > 0 else 0.0
        results["line_f1"] = f1

    if "exact_match_rate" in metrics_cfg:
        results["exact_match_rate"] = exact_match_count / len(samples)

    return results
